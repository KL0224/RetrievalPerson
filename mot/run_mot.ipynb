{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19882e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:50:11.178093Z",
     "iopub.status.busy": "2026-01-11T16:50:11.177766Z",
     "iopub.status.idle": "2026-01-11T16:50:12.952932Z",
     "shell.execute_reply": "2026-01-11T16:50:12.952260Z"
    },
    "papermill": {
     "duration": 1.781143,
     "end_time": "2026-01-11T16:50:12.954660",
     "exception": false,
     "start_time": "2026-01-11T16:50:11.173517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RetrievalPerson'...\r\n",
      "remote: Enumerating objects: 174, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (174/174), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (112/112), done.\u001b[K\r\n",
      "remote: Total 174 (delta 68), reused 154 (delta 48), pack-reused 0 (from 0)\u001b[K\r\n",
      "Receiving objects: 100% (174/174), 18.64 MiB | 41.48 MiB/s, done.\r\n",
      "Resolving deltas: 100% (68/68), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/KL0224/RetrievalPerson -b new_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61f52df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:50:12.961588Z",
     "iopub.status.busy": "2026-01-11T16:50:12.961329Z",
     "iopub.status.idle": "2026-01-11T16:50:36.936281Z",
     "shell.execute_reply": "2026-01-11T16:50:36.935459Z"
    },
    "papermill": {
     "duration": 23.980515,
     "end_time": "2026-01-11T16:50:36.938172",
     "exception": false,
     "start_time": "2026-01-11T16:50:12.957657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting open_clip_torch==3.2.0\r\n",
      "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\r\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (2.8.0+cu126)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (0.23.0+cu126)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (2025.11.3)\r\n",
      "Collecting ftfy (from open_clip_torch==3.2.0)\r\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (4.67.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (0.36.0)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (0.6.2)\r\n",
      "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (1.0.20)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch==3.2.0) (6.0.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (3.4.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch==3.2.0) (0.2.14)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch==3.2.0) (25.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch==3.2.0) (2.32.5)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch==3.2.0) (1.2.1rc0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch==3.2.0) (2.0.2)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch==3.2.0) (11.3.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch==3.2.0) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch==3.2.0) (3.0.3)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch==3.2.0) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch==3.2.0) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch==3.2.0) (2.6.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch==3.2.0) (2025.11.12)\r\n",
      "Downloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ftfy, open_clip_torch\r\n",
      "Successfully installed ftfy-6.3.1 open_clip_torch-3.2.0\r\n",
      "Collecting torchreid==0.2.5\r\n",
      "  Downloading torchreid-0.2.5.tar.gz (92 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Building wheels for collected packages: torchreid\r\n",
      "  Building wheel for torchreid (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for torchreid: filename=torchreid-0.2.5-py3-none-any.whl size=144324 sha256=a8727f5ac4e4e92bd6fca3a402b417b025bc2d0f82575eb6803fcb0e8c1ceaad\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/86/ff/80a1b78a90df470cbb12c075bf189ad33f1a41a881cf9e9a09\r\n",
      "Successfully built torchreid\r\n",
      "Installing collected packages: torchreid\r\n",
      "Successfully installed torchreid-0.2.5\r\n",
      "Collecting ultralytics==8.3.243\r\n",
      "  Downloading ultralytics-8.3.243-py3-none-any.whl.metadata (37 kB)\r\n",
      "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (2.0.2)\r\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (3.10.0)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (4.12.0.88)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (11.3.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (6.0.3)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (2.32.5)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (1.15.3)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (2.8.0+cu126)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (0.23.0+cu126)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (5.9.5)\r\n",
      "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.243) (1.25.2)\r\n",
      "Collecting ultralytics-thop>=2.0.18 (from ultralytics==8.3.243)\r\n",
      "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.243) (1.3.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.243) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.243) (4.60.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.243) (1.4.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.243) (25.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.243) (3.2.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.243) (2.9.0.post0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.243) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.243) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.243) (2.6.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.243) (2025.11.12)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.243) (3.4.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.3.243) (1.17.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics==8.3.243) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.3.243) (3.0.3)\r\n",
      "Downloading ultralytics-8.3.243-py3-none-any.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\r\n",
      "Installing collected packages: ultralytics-thop, ultralytics\r\n",
      "Successfully installed ultralytics-8.3.243 ultralytics-thop-2.0.18\r\n",
      "Collecting deep-sort-realtime\r\n",
      "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deep-sort-realtime) (2.0.2)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from deep-sort-realtime) (1.15.3)\r\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from deep-sort-realtime) (4.12.0.88)\r\n",
      "Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: deep-sort-realtime\r\n",
      "Successfully installed deep-sort-realtime-1.3.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install open_clip_torch==3.2.0\n",
    "!pip install torchreid==0.2.5\n",
    "!pip install lap>=0.5.12\n",
    "!pip install ultralytics==8.3.243\n",
    "!pip install deep-sort-realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe7769a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:50:36.948307Z",
     "iopub.status.busy": "2026-01-11T16:50:36.948000Z",
     "iopub.status.idle": "2026-01-11T16:50:36.951847Z",
     "shell.execute_reply": "2026-01-11T16:50:36.951232Z"
    },
    "papermill": {
     "duration": 0.010636,
     "end_time": "2026-01-11T16:50:36.953336",
     "exception": false,
     "start_time": "2026-01-11T16:50:36.942700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/RetrievalPerson/deep_person_reid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d991d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:50:36.963185Z",
     "iopub.status.busy": "2026-01-11T16:50:36.962907Z",
     "iopub.status.idle": "2026-01-11T16:50:36.969005Z",
     "shell.execute_reply": "2026-01-11T16:50:36.968153Z"
    },
    "papermill": {
     "duration": 0.012583,
     "end_time": "2026-01-11T16:50:36.970376",
     "exception": false,
     "start_time": "2026-01-11T16:50:36.957793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/RetrievalPerson\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/RetrievalPerson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9da3a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:50:36.979804Z",
     "iopub.status.busy": "2026-01-11T16:50:36.979602Z",
     "iopub.status.idle": "2026-01-11T16:50:37.097400Z",
     "shell.execute_reply": "2026-01-11T16:50:37.096338Z"
    },
    "papermill": {
     "duration": 0.124656,
     "end_time": "2026-01-11T16:50:37.099379",
     "exception": false,
     "start_time": "2026-01-11T16:50:36.974723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf deep_person_reid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd1a089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:50:37.110093Z",
     "iopub.status.busy": "2026-01-11T16:50:37.109598Z",
     "iopub.status.idle": "2026-01-11T16:50:37.858834Z",
     "shell.execute_reply": "2026-01-11T16:50:37.858119Z"
    },
    "papermill": {
     "duration": 0.756799,
     "end_time": "2026-01-11T16:50:37.860782",
     "exception": false,
     "start_time": "2026-01-11T16:50:37.103983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'deep_person_reid'...\r\n",
      "remote: Enumerating objects: 5040, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (773/773), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (107/107), done.\u001b[K\r\n",
      "remote: Total 5040 (delta 698), reused 667 (delta 666), pack-reused 4267 (from 2)\u001b[K\r\n",
      "Receiving objects: 100% (5040/5040), 1.83 MiB | 19.96 MiB/s, done.\r\n",
      "Resolving deltas: 100% (3530/3530), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/baotodale06/deep_person_reid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d262b790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:50:37.871883Z",
     "iopub.status.busy": "2026-01-11T16:50:37.871631Z",
     "iopub.status.idle": "2026-01-11T16:50:37.877495Z",
     "shell.execute_reply": "2026-01-11T16:50:37.877007Z"
    },
    "papermill": {
     "duration": 0.013165,
     "end_time": "2026-01-11T16:50:37.879013",
     "exception": false,
     "start_time": "2026-01-11T16:50:37.865848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "VIDEO_FOLDER = '/kaggle/input/dataset-person/videos'\n",
    "OUTPUT_FOLDER = '/kaggle/working/outputs'\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "os.makedirs(os.path.join(OUTPUT_FOLDER, 'frames'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_FOLDER, 'annotated_videos'), exist_ok=True)\n",
    "\n",
    "\n",
    "crops_folder = os.path.join(OUTPUT_FOLDER, 'crops')\n",
    "os.makedirs(crops_folder, exist_ok=True)\n",
    "\n",
    "metadata_folder = os.path.join(OUTPUT_FOLDER, 'metadata')\n",
    "os.makedirs(metadata_folder, exist_ok=True)\n",
    "\n",
    "features_folder = os.path.join(OUTPUT_FOLDER, 'features')\n",
    "os.makedirs(features_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0a44651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:50:37.889348Z",
     "iopub.status.busy": "2026-01-11T16:50:37.889131Z",
     "iopub.status.idle": "2026-01-11T16:51:23.640272Z",
     "shell.execute_reply": "2026-01-11T16:51:23.639493Z"
    },
    "papermill": {
     "duration": 45.758608,
     "end_time": "2026-01-11T16:51:23.642164",
     "exception": false,
     "start_time": "2026-01-11T16:50:37.883556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchreid/reid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768150256.893309      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768150256.994733      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768150257.828387      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768150257.828439      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768150257.828442      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768150257.828444      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from config import *\n",
    "from tracking.tracklet import TrackletManager\n",
    "from tracking.detector_tracker import run_tracking\n",
    "from sampling.sampler import sample_best_per_window\n",
    "from models.reid import ReIDModel\n",
    "from models.clip_model import CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c710da53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:51:23.655995Z",
     "iopub.status.busy": "2026-01-11T16:51:23.655247Z",
     "iopub.status.idle": "2026-01-11T16:51:23.661624Z",
     "shell.execute_reply": "2026-01-11T16:51:23.661025Z"
    },
    "papermill": {
     "duration": 0.014036,
     "end_time": "2026-01-11T16:51:23.662918",
     "exception": false,
     "start_time": "2026-01-11T16:51:23.648882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_image_webp(img_bgr, path: str, quality: int = 80, resize_factor: float = 0.5):\n",
    "    if resize_factor != 1.0:\n",
    "        img_bgr = cv2.resize(img_bgr, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    img_pil.save(path, format=\"WEBP\", quality=quality)\n",
    "\n",
    "def save_crop_webp(crop, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    cv2.imwrite(\n",
    "        path,\n",
    "        crop,\n",
    "        [cv2.IMWRITE_WEBP_QUALITY, 100]  # 100 = lossless\n",
    "    )\n",
    "\n",
    "def safe_delete(path):\n",
    "    try:\n",
    "        if path and os.path.exists(path):\n",
    "            os.remove(path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to delete {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "115ebeb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:51:23.674803Z",
     "iopub.status.busy": "2026-01-11T16:51:23.674556Z",
     "iopub.status.idle": "2026-01-11T16:51:23.685114Z",
     "shell.execute_reply": "2026-01-11T16:51:23.684522Z"
    },
    "papermill": {
     "duration": 0.018371,
     "end_time": "2026-01-11T16:51:23.686540",
     "exception": false,
     "start_time": "2026-01-11T16:51:23.668169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import os\n",
    "\n",
    "def visualize_video_with_ids(\n",
    "    video_path,\n",
    "    tracks_per_frame,\n",
    "    output_path=None\n",
    "):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "\n",
    "    writer = None\n",
    "    if output_path:\n",
    "        ext = os.path.splitext(output_path)[1].lower()\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "        \n",
    "        # if ext == \".avi\":\n",
    "        #     fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "        # elif ext == \".mp4\":\n",
    "        #     fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        # else:\n",
    "        #     raise ValueError(\"Unsupported output format. Use .avi or .mp4\")\n",
    "\n",
    "        writer = cv2.VideoWriter(\n",
    "            output_path,\n",
    "            fourcc,\n",
    "            fps,\n",
    "            (width, height)\n",
    "        )\n",
    "\n",
    "    id_colors = {}\n",
    "    frame_idx = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        for item in tracks_per_frame.get(frame_idx, []):\n",
    "            track_id = item[\"id\"]\n",
    "            x1, y1, x2, y2 = map(int, item[\"bbox\"])\n",
    "\n",
    "            if track_id not in id_colors:\n",
    "                random.seed(int(track_id))\n",
    "                id_colors[track_id] = (\n",
    "                    random.randint(50, 255),\n",
    "                    random.randint(50, 255),\n",
    "                    random.randint(50, 255),\n",
    "                )\n",
    "\n",
    "            color = id_colors[track_id]\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"ID {track_id}\",\n",
    "                (x1, max(0, y1 - 7)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                color,\n",
    "                2\n",
    "            )\n",
    "\n",
    "        if writer:\n",
    "            writer.write(frame)\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    if writer:\n",
    "        writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf76e032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:51:23.697770Z",
     "iopub.status.busy": "2026-01-11T16:51:23.697509Z",
     "iopub.status.idle": "2026-01-11T16:51:23.755065Z",
     "shell.execute_reply": "2026-01-11T16:51:23.754262Z"
    },
    "papermill": {
     "duration": 0.064907,
     "end_time": "2026-01-11T16:51:23.756422",
     "exception": false,
     "start_time": "2026-01-11T16:51:23.691515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc93356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:51:23.767986Z",
     "iopub.status.busy": "2026-01-11T16:51:23.767713Z",
     "iopub.status.idle": "2026-01-11T20:15:46.335588Z",
     "shell.execute_reply": "2026-01-11T20:15:46.334644Z"
    },
    "papermill": {
     "duration": 12262.576246,
     "end_time": "2026-01-11T20:15:46.337736",
     "exception": false,
     "start_time": "2026-01-11T16:51:23.761490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n",
      "Successfully loaded pretrained weights from \"./models/osnet_ain_x1_0_dukemtmcreid_256x128_amsgrad_ep90_lr0.0015_coslr_b64_fb10_softmax_labsmth_flip_jitter.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['conv2.0.conv2.0.layers.0.conv1.weight', 'conv2.0.conv2.0.layers.0.conv2.weight', 'conv2.0.conv2.0.layers.0.bn.weight', 'conv2.0.conv2.0.layers.0.bn.bias', 'conv2.0.conv2.0.layers.0.bn.running_mean', 'conv2.0.conv2.0.layers.0.bn.running_var', 'conv2.0.conv2.0.layers.0.bn.num_batches_tracked', 'conv2.0.conv2.1.layers.0.conv1.weight', 'conv2.0.conv2.1.layers.0.conv2.weight', 'conv2.0.conv2.1.layers.0.bn.weight', 'conv2.0.conv2.1.layers.0.bn.bias', 'conv2.0.conv2.1.layers.0.bn.running_mean', 'conv2.0.conv2.1.layers.0.bn.running_var', 'conv2.0.conv2.1.layers.0.bn.num_batches_tracked', 'conv2.0.conv2.1.layers.1.conv1.weight', 'conv2.0.conv2.1.layers.1.conv2.weight', 'conv2.0.conv2.1.layers.1.bn.weight', 'conv2.0.conv2.1.layers.1.bn.bias', 'conv2.0.conv2.1.layers.1.bn.running_mean', 'conv2.0.conv2.1.layers.1.bn.running_var', 'conv2.0.conv2.1.layers.1.bn.num_batches_tracked', 'conv2.0.conv2.2.layers.0.conv1.weight', 'conv2.0.conv2.2.layers.0.conv2.weight', 'conv2.0.conv2.2.layers.0.bn.weight', 'conv2.0.conv2.2.layers.0.bn.bias', 'conv2.0.conv2.2.layers.0.bn.running_mean', 'conv2.0.conv2.2.layers.0.bn.running_var', 'conv2.0.conv2.2.layers.0.bn.num_batches_tracked', 'conv2.0.conv2.2.layers.1.conv1.weight', 'conv2.0.conv2.2.layers.1.conv2.weight', 'conv2.0.conv2.2.layers.1.bn.weight', 'conv2.0.conv2.2.layers.1.bn.bias', 'conv2.0.conv2.2.layers.1.bn.running_mean', 'conv2.0.conv2.2.layers.1.bn.running_var', 'conv2.0.conv2.2.layers.1.bn.num_batches_tracked', 'conv2.0.conv2.2.layers.2.conv1.weight', 'conv2.0.conv2.2.layers.2.conv2.weight', 'conv2.0.conv2.2.layers.2.bn.weight', 'conv2.0.conv2.2.layers.2.bn.bias', 'conv2.0.conv2.2.layers.2.bn.running_mean', 'conv2.0.conv2.2.layers.2.bn.running_var', 'conv2.0.conv2.2.layers.2.bn.num_batches_tracked', 'conv2.0.conv2.3.layers.0.conv1.weight', 'conv2.0.conv2.3.layers.0.conv2.weight', 'conv2.0.conv2.3.layers.0.bn.weight', 'conv2.0.conv2.3.layers.0.bn.bias', 'conv2.0.conv2.3.layers.0.bn.running_mean', 'conv2.0.conv2.3.layers.0.bn.running_var', 'conv2.0.conv2.3.layers.0.bn.num_batches_tracked', 'conv2.0.conv2.3.layers.1.conv1.weight', 'conv2.0.conv2.3.layers.1.conv2.weight', 'conv2.0.conv2.3.layers.1.bn.weight', 'conv2.0.conv2.3.layers.1.bn.bias', 'conv2.0.conv2.3.layers.1.bn.running_mean', 'conv2.0.conv2.3.layers.1.bn.running_var', 'conv2.0.conv2.3.layers.1.bn.num_batches_tracked', 'conv2.0.conv2.3.layers.2.conv1.weight', 'conv2.0.conv2.3.layers.2.conv2.weight', 'conv2.0.conv2.3.layers.2.bn.weight', 'conv2.0.conv2.3.layers.2.bn.bias', 'conv2.0.conv2.3.layers.2.bn.running_mean', 'conv2.0.conv2.3.layers.2.bn.running_var', 'conv2.0.conv2.3.layers.2.bn.num_batches_tracked', 'conv2.0.conv2.3.layers.3.conv1.weight', 'conv2.0.conv2.3.layers.3.conv2.weight', 'conv2.0.conv2.3.layers.3.bn.weight', 'conv2.0.conv2.3.layers.3.bn.bias', 'conv2.0.conv2.3.layers.3.bn.running_mean', 'conv2.0.conv2.3.layers.3.bn.running_var', 'conv2.0.conv2.3.layers.3.bn.num_batches_tracked', 'conv2.0.IN.weight', 'conv2.0.IN.bias', 'conv2.1.conv2.0.layers.0.conv1.weight', 'conv2.1.conv2.0.layers.0.conv2.weight', 'conv2.1.conv2.0.layers.0.bn.weight', 'conv2.1.conv2.0.layers.0.bn.bias', 'conv2.1.conv2.0.layers.0.bn.running_mean', 'conv2.1.conv2.0.layers.0.bn.running_var', 'conv2.1.conv2.0.layers.0.bn.num_batches_tracked', 'conv2.1.conv2.1.layers.0.conv1.weight', 'conv2.1.conv2.1.layers.0.conv2.weight', 'conv2.1.conv2.1.layers.0.bn.weight', 'conv2.1.conv2.1.layers.0.bn.bias', 'conv2.1.conv2.1.layers.0.bn.running_mean', 'conv2.1.conv2.1.layers.0.bn.running_var', 'conv2.1.conv2.1.layers.0.bn.num_batches_tracked', 'conv2.1.conv2.1.layers.1.conv1.weight', 'conv2.1.conv2.1.layers.1.conv2.weight', 'conv2.1.conv2.1.layers.1.bn.weight', 'conv2.1.conv2.1.layers.1.bn.bias', 'conv2.1.conv2.1.layers.1.bn.running_mean', 'conv2.1.conv2.1.layers.1.bn.running_var', 'conv2.1.conv2.1.layers.1.bn.num_batches_tracked', 'conv2.1.conv2.2.layers.0.conv1.weight', 'conv2.1.conv2.2.layers.0.conv2.weight', 'conv2.1.conv2.2.layers.0.bn.weight', 'conv2.1.conv2.2.layers.0.bn.bias', 'conv2.1.conv2.2.layers.0.bn.running_mean', 'conv2.1.conv2.2.layers.0.bn.running_var', 'conv2.1.conv2.2.layers.0.bn.num_batches_tracked', 'conv2.1.conv2.2.layers.1.conv1.weight', 'conv2.1.conv2.2.layers.1.conv2.weight', 'conv2.1.conv2.2.layers.1.bn.weight', 'conv2.1.conv2.2.layers.1.bn.bias', 'conv2.1.conv2.2.layers.1.bn.running_mean', 'conv2.1.conv2.2.layers.1.bn.running_var', 'conv2.1.conv2.2.layers.1.bn.num_batches_tracked', 'conv2.1.conv2.2.layers.2.conv1.weight', 'conv2.1.conv2.2.layers.2.conv2.weight', 'conv2.1.conv2.2.layers.2.bn.weight', 'conv2.1.conv2.2.layers.2.bn.bias', 'conv2.1.conv2.2.layers.2.bn.running_mean', 'conv2.1.conv2.2.layers.2.bn.running_var', 'conv2.1.conv2.2.layers.2.bn.num_batches_tracked', 'conv2.1.conv2.3.layers.0.conv1.weight', 'conv2.1.conv2.3.layers.0.conv2.weight', 'conv2.1.conv2.3.layers.0.bn.weight', 'conv2.1.conv2.3.layers.0.bn.bias', 'conv2.1.conv2.3.layers.0.bn.running_mean', 'conv2.1.conv2.3.layers.0.bn.running_var', 'conv2.1.conv2.3.layers.0.bn.num_batches_tracked', 'conv2.1.conv2.3.layers.1.conv1.weight', 'conv2.1.conv2.3.layers.1.conv2.weight', 'conv2.1.conv2.3.layers.1.bn.weight', 'conv2.1.conv2.3.layers.1.bn.bias', 'conv2.1.conv2.3.layers.1.bn.running_mean', 'conv2.1.conv2.3.layers.1.bn.running_var', 'conv2.1.conv2.3.layers.1.bn.num_batches_tracked', 'conv2.1.conv2.3.layers.2.conv1.weight', 'conv2.1.conv2.3.layers.2.conv2.weight', 'conv2.1.conv2.3.layers.2.bn.weight', 'conv2.1.conv2.3.layers.2.bn.bias', 'conv2.1.conv2.3.layers.2.bn.running_mean', 'conv2.1.conv2.3.layers.2.bn.running_var', 'conv2.1.conv2.3.layers.2.bn.num_batches_tracked', 'conv2.1.conv2.3.layers.3.conv1.weight', 'conv2.1.conv2.3.layers.3.conv2.weight', 'conv2.1.conv2.3.layers.3.bn.weight', 'conv2.1.conv2.3.layers.3.bn.bias', 'conv2.1.conv2.3.layers.3.bn.running_mean', 'conv2.1.conv2.3.layers.3.bn.running_var', 'conv2.1.conv2.3.layers.3.bn.num_batches_tracked', 'conv2.1.IN.weight', 'conv2.1.IN.bias', 'pool2.0.conv.weight', 'pool2.0.bn.weight', 'pool2.0.bn.bias', 'pool2.0.bn.running_mean', 'pool2.0.bn.running_var', 'pool2.0.bn.num_batches_tracked', 'conv3.0.conv2.0.layers.0.conv1.weight', 'conv3.0.conv2.0.layers.0.conv2.weight', 'conv3.0.conv2.0.layers.0.bn.weight', 'conv3.0.conv2.0.layers.0.bn.bias', 'conv3.0.conv2.0.layers.0.bn.running_mean', 'conv3.0.conv2.0.layers.0.bn.running_var', 'conv3.0.conv2.0.layers.0.bn.num_batches_tracked', 'conv3.0.conv2.1.layers.0.conv1.weight', 'conv3.0.conv2.1.layers.0.conv2.weight', 'conv3.0.conv2.1.layers.0.bn.weight', 'conv3.0.conv2.1.layers.0.bn.bias', 'conv3.0.conv2.1.layers.0.bn.running_mean', 'conv3.0.conv2.1.layers.0.bn.running_var', 'conv3.0.conv2.1.layers.0.bn.num_batches_tracked', 'conv3.0.conv2.1.layers.1.conv1.weight', 'conv3.0.conv2.1.layers.1.conv2.weight', 'conv3.0.conv2.1.layers.1.bn.weight', 'conv3.0.conv2.1.layers.1.bn.bias', 'conv3.0.conv2.1.layers.1.bn.running_mean', 'conv3.0.conv2.1.layers.1.bn.running_var', 'conv3.0.conv2.1.layers.1.bn.num_batches_tracked', 'conv3.0.conv2.2.layers.0.conv1.weight', 'conv3.0.conv2.2.layers.0.conv2.weight', 'conv3.0.conv2.2.layers.0.bn.weight', 'conv3.0.conv2.2.layers.0.bn.bias', 'conv3.0.conv2.2.layers.0.bn.running_mean', 'conv3.0.conv2.2.layers.0.bn.running_var', 'conv3.0.conv2.2.layers.0.bn.num_batches_tracked', 'conv3.0.conv2.2.layers.1.conv1.weight', 'conv3.0.conv2.2.layers.1.conv2.weight', 'conv3.0.conv2.2.layers.1.bn.weight', 'conv3.0.conv2.2.layers.1.bn.bias', 'conv3.0.conv2.2.layers.1.bn.running_mean', 'conv3.0.conv2.2.layers.1.bn.running_var', 'conv3.0.conv2.2.layers.1.bn.num_batches_tracked', 'conv3.0.conv2.2.layers.2.conv1.weight', 'conv3.0.conv2.2.layers.2.conv2.weight', 'conv3.0.conv2.2.layers.2.bn.weight', 'conv3.0.conv2.2.layers.2.bn.bias', 'conv3.0.conv2.2.layers.2.bn.running_mean', 'conv3.0.conv2.2.layers.2.bn.running_var', 'conv3.0.conv2.2.layers.2.bn.num_batches_tracked', 'conv3.0.conv2.3.layers.0.conv1.weight', 'conv3.0.conv2.3.layers.0.conv2.weight', 'conv3.0.conv2.3.layers.0.bn.weight', 'conv3.0.conv2.3.layers.0.bn.bias', 'conv3.0.conv2.3.layers.0.bn.running_mean', 'conv3.0.conv2.3.layers.0.bn.running_var', 'conv3.0.conv2.3.layers.0.bn.num_batches_tracked', 'conv3.0.conv2.3.layers.1.conv1.weight', 'conv3.0.conv2.3.layers.1.conv2.weight', 'conv3.0.conv2.3.layers.1.bn.weight', 'conv3.0.conv2.3.layers.1.bn.bias', 'conv3.0.conv2.3.layers.1.bn.running_mean', 'conv3.0.conv2.3.layers.1.bn.running_var', 'conv3.0.conv2.3.layers.1.bn.num_batches_tracked', 'conv3.0.conv2.3.layers.2.conv1.weight', 'conv3.0.conv2.3.layers.2.conv2.weight', 'conv3.0.conv2.3.layers.2.bn.weight', 'conv3.0.conv2.3.layers.2.bn.bias', 'conv3.0.conv2.3.layers.2.bn.running_mean', 'conv3.0.conv2.3.layers.2.bn.running_var', 'conv3.0.conv2.3.layers.2.bn.num_batches_tracked', 'conv3.0.conv2.3.layers.3.conv1.weight', 'conv3.0.conv2.3.layers.3.conv2.weight', 'conv3.0.conv2.3.layers.3.bn.weight', 'conv3.0.conv2.3.layers.3.bn.bias', 'conv3.0.conv2.3.layers.3.bn.running_mean', 'conv3.0.conv2.3.layers.3.bn.running_var', 'conv3.0.conv2.3.layers.3.bn.num_batches_tracked', 'conv3.1.conv2.0.layers.0.conv1.weight', 'conv3.1.conv2.0.layers.0.conv2.weight', 'conv3.1.conv2.0.layers.0.bn.weight', 'conv3.1.conv2.0.layers.0.bn.bias', 'conv3.1.conv2.0.layers.0.bn.running_mean', 'conv3.1.conv2.0.layers.0.bn.running_var', 'conv3.1.conv2.0.layers.0.bn.num_batches_tracked', 'conv3.1.conv2.1.layers.0.conv1.weight', 'conv3.1.conv2.1.layers.0.conv2.weight', 'conv3.1.conv2.1.layers.0.bn.weight', 'conv3.1.conv2.1.layers.0.bn.bias', 'conv3.1.conv2.1.layers.0.bn.running_mean', 'conv3.1.conv2.1.layers.0.bn.running_var', 'conv3.1.conv2.1.layers.0.bn.num_batches_tracked', 'conv3.1.conv2.1.layers.1.conv1.weight', 'conv3.1.conv2.1.layers.1.conv2.weight', 'conv3.1.conv2.1.layers.1.bn.weight', 'conv3.1.conv2.1.layers.1.bn.bias', 'conv3.1.conv2.1.layers.1.bn.running_mean', 'conv3.1.conv2.1.layers.1.bn.running_var', 'conv3.1.conv2.1.layers.1.bn.num_batches_tracked', 'conv3.1.conv2.2.layers.0.conv1.weight', 'conv3.1.conv2.2.layers.0.conv2.weight', 'conv3.1.conv2.2.layers.0.bn.weight', 'conv3.1.conv2.2.layers.0.bn.bias', 'conv3.1.conv2.2.layers.0.bn.running_mean', 'conv3.1.conv2.2.layers.0.bn.running_var', 'conv3.1.conv2.2.layers.0.bn.num_batches_tracked', 'conv3.1.conv2.2.layers.1.conv1.weight', 'conv3.1.conv2.2.layers.1.conv2.weight', 'conv3.1.conv2.2.layers.1.bn.weight', 'conv3.1.conv2.2.layers.1.bn.bias', 'conv3.1.conv2.2.layers.1.bn.running_mean', 'conv3.1.conv2.2.layers.1.bn.running_var', 'conv3.1.conv2.2.layers.1.bn.num_batches_tracked', 'conv3.1.conv2.2.layers.2.conv1.weight', 'conv3.1.conv2.2.layers.2.conv2.weight', 'conv3.1.conv2.2.layers.2.bn.weight', 'conv3.1.conv2.2.layers.2.bn.bias', 'conv3.1.conv2.2.layers.2.bn.running_mean', 'conv3.1.conv2.2.layers.2.bn.running_var', 'conv3.1.conv2.2.layers.2.bn.num_batches_tracked', 'conv3.1.conv2.3.layers.0.conv1.weight', 'conv3.1.conv2.3.layers.0.conv2.weight', 'conv3.1.conv2.3.layers.0.bn.weight', 'conv3.1.conv2.3.layers.0.bn.bias', 'conv3.1.conv2.3.layers.0.bn.running_mean', 'conv3.1.conv2.3.layers.0.bn.running_var', 'conv3.1.conv2.3.layers.0.bn.num_batches_tracked', 'conv3.1.conv2.3.layers.1.conv1.weight', 'conv3.1.conv2.3.layers.1.conv2.weight', 'conv3.1.conv2.3.layers.1.bn.weight', 'conv3.1.conv2.3.layers.1.bn.bias', 'conv3.1.conv2.3.layers.1.bn.running_mean', 'conv3.1.conv2.3.layers.1.bn.running_var', 'conv3.1.conv2.3.layers.1.bn.num_batches_tracked', 'conv3.1.conv2.3.layers.2.conv1.weight', 'conv3.1.conv2.3.layers.2.conv2.weight', 'conv3.1.conv2.3.layers.2.bn.weight', 'conv3.1.conv2.3.layers.2.bn.bias', 'conv3.1.conv2.3.layers.2.bn.running_mean', 'conv3.1.conv2.3.layers.2.bn.running_var', 'conv3.1.conv2.3.layers.2.bn.num_batches_tracked', 'conv3.1.conv2.3.layers.3.conv1.weight', 'conv3.1.conv2.3.layers.3.conv2.weight', 'conv3.1.conv2.3.layers.3.bn.weight', 'conv3.1.conv2.3.layers.3.bn.bias', 'conv3.1.conv2.3.layers.3.bn.running_mean', 'conv3.1.conv2.3.layers.3.bn.running_var', 'conv3.1.conv2.3.layers.3.bn.num_batches_tracked', 'conv3.1.IN.weight', 'conv3.1.IN.bias', 'pool3.0.conv.weight', 'pool3.0.bn.weight', 'pool3.0.bn.bias', 'pool3.0.bn.running_mean', 'pool3.0.bn.running_var', 'pool3.0.bn.num_batches_tracked', 'conv4.0.conv2.0.layers.0.conv1.weight', 'conv4.0.conv2.0.layers.0.conv2.weight', 'conv4.0.conv2.0.layers.0.bn.weight', 'conv4.0.conv2.0.layers.0.bn.bias', 'conv4.0.conv2.0.layers.0.bn.running_mean', 'conv4.0.conv2.0.layers.0.bn.running_var', 'conv4.0.conv2.0.layers.0.bn.num_batches_tracked', 'conv4.0.conv2.1.layers.0.conv1.weight', 'conv4.0.conv2.1.layers.0.conv2.weight', 'conv4.0.conv2.1.layers.0.bn.weight', 'conv4.0.conv2.1.layers.0.bn.bias', 'conv4.0.conv2.1.layers.0.bn.running_mean', 'conv4.0.conv2.1.layers.0.bn.running_var', 'conv4.0.conv2.1.layers.0.bn.num_batches_tracked', 'conv4.0.conv2.1.layers.1.conv1.weight', 'conv4.0.conv2.1.layers.1.conv2.weight', 'conv4.0.conv2.1.layers.1.bn.weight', 'conv4.0.conv2.1.layers.1.bn.bias', 'conv4.0.conv2.1.layers.1.bn.running_mean', 'conv4.0.conv2.1.layers.1.bn.running_var', 'conv4.0.conv2.1.layers.1.bn.num_batches_tracked', 'conv4.0.conv2.2.layers.0.conv1.weight', 'conv4.0.conv2.2.layers.0.conv2.weight', 'conv4.0.conv2.2.layers.0.bn.weight', 'conv4.0.conv2.2.layers.0.bn.bias', 'conv4.0.conv2.2.layers.0.bn.running_mean', 'conv4.0.conv2.2.layers.0.bn.running_var', 'conv4.0.conv2.2.layers.0.bn.num_batches_tracked', 'conv4.0.conv2.2.layers.1.conv1.weight', 'conv4.0.conv2.2.layers.1.conv2.weight', 'conv4.0.conv2.2.layers.1.bn.weight', 'conv4.0.conv2.2.layers.1.bn.bias', 'conv4.0.conv2.2.layers.1.bn.running_mean', 'conv4.0.conv2.2.layers.1.bn.running_var', 'conv4.0.conv2.2.layers.1.bn.num_batches_tracked', 'conv4.0.conv2.2.layers.2.conv1.weight', 'conv4.0.conv2.2.layers.2.conv2.weight', 'conv4.0.conv2.2.layers.2.bn.weight', 'conv4.0.conv2.2.layers.2.bn.bias', 'conv4.0.conv2.2.layers.2.bn.running_mean', 'conv4.0.conv2.2.layers.2.bn.running_var', 'conv4.0.conv2.2.layers.2.bn.num_batches_tracked', 'conv4.0.conv2.3.layers.0.conv1.weight', 'conv4.0.conv2.3.layers.0.conv2.weight', 'conv4.0.conv2.3.layers.0.bn.weight', 'conv4.0.conv2.3.layers.0.bn.bias', 'conv4.0.conv2.3.layers.0.bn.running_mean', 'conv4.0.conv2.3.layers.0.bn.running_var', 'conv4.0.conv2.3.layers.0.bn.num_batches_tracked', 'conv4.0.conv2.3.layers.1.conv1.weight', 'conv4.0.conv2.3.layers.1.conv2.weight', 'conv4.0.conv2.3.layers.1.bn.weight', 'conv4.0.conv2.3.layers.1.bn.bias', 'conv4.0.conv2.3.layers.1.bn.running_mean', 'conv4.0.conv2.3.layers.1.bn.running_var', 'conv4.0.conv2.3.layers.1.bn.num_batches_tracked', 'conv4.0.conv2.3.layers.2.conv1.weight', 'conv4.0.conv2.3.layers.2.conv2.weight', 'conv4.0.conv2.3.layers.2.bn.weight', 'conv4.0.conv2.3.layers.2.bn.bias', 'conv4.0.conv2.3.layers.2.bn.running_mean', 'conv4.0.conv2.3.layers.2.bn.running_var', 'conv4.0.conv2.3.layers.2.bn.num_batches_tracked', 'conv4.0.conv2.3.layers.3.conv1.weight', 'conv4.0.conv2.3.layers.3.conv2.weight', 'conv4.0.conv2.3.layers.3.bn.weight', 'conv4.0.conv2.3.layers.3.bn.bias', 'conv4.0.conv2.3.layers.3.bn.running_mean', 'conv4.0.conv2.3.layers.3.bn.running_var', 'conv4.0.conv2.3.layers.3.bn.num_batches_tracked', 'conv4.0.IN.weight', 'conv4.0.IN.bias', 'conv4.1.conv2.0.layers.0.conv1.weight', 'conv4.1.conv2.0.layers.0.conv2.weight', 'conv4.1.conv2.0.layers.0.bn.weight', 'conv4.1.conv2.0.layers.0.bn.bias', 'conv4.1.conv2.0.layers.0.bn.running_mean', 'conv4.1.conv2.0.layers.0.bn.running_var', 'conv4.1.conv2.0.layers.0.bn.num_batches_tracked', 'conv4.1.conv2.1.layers.0.conv1.weight', 'conv4.1.conv2.1.layers.0.conv2.weight', 'conv4.1.conv2.1.layers.0.bn.weight', 'conv4.1.conv2.1.layers.0.bn.bias', 'conv4.1.conv2.1.layers.0.bn.running_mean', 'conv4.1.conv2.1.layers.0.bn.running_var', 'conv4.1.conv2.1.layers.0.bn.num_batches_tracked', 'conv4.1.conv2.1.layers.1.conv1.weight', 'conv4.1.conv2.1.layers.1.conv2.weight', 'conv4.1.conv2.1.layers.1.bn.weight', 'conv4.1.conv2.1.layers.1.bn.bias', 'conv4.1.conv2.1.layers.1.bn.running_mean', 'conv4.1.conv2.1.layers.1.bn.running_var', 'conv4.1.conv2.1.layers.1.bn.num_batches_tracked', 'conv4.1.conv2.2.layers.0.conv1.weight', 'conv4.1.conv2.2.layers.0.conv2.weight', 'conv4.1.conv2.2.layers.0.bn.weight', 'conv4.1.conv2.2.layers.0.bn.bias', 'conv4.1.conv2.2.layers.0.bn.running_mean', 'conv4.1.conv2.2.layers.0.bn.running_var', 'conv4.1.conv2.2.layers.0.bn.num_batches_tracked', 'conv4.1.conv2.2.layers.1.conv1.weight', 'conv4.1.conv2.2.layers.1.conv2.weight', 'conv4.1.conv2.2.layers.1.bn.weight', 'conv4.1.conv2.2.layers.1.bn.bias', 'conv4.1.conv2.2.layers.1.bn.running_mean', 'conv4.1.conv2.2.layers.1.bn.running_var', 'conv4.1.conv2.2.layers.1.bn.num_batches_tracked', 'conv4.1.conv2.2.layers.2.conv1.weight', 'conv4.1.conv2.2.layers.2.conv2.weight', 'conv4.1.conv2.2.layers.2.bn.weight', 'conv4.1.conv2.2.layers.2.bn.bias', 'conv4.1.conv2.2.layers.2.bn.running_mean', 'conv4.1.conv2.2.layers.2.bn.running_var', 'conv4.1.conv2.2.layers.2.bn.num_batches_tracked', 'conv4.1.conv2.3.layers.0.conv1.weight', 'conv4.1.conv2.3.layers.0.conv2.weight', 'conv4.1.conv2.3.layers.0.bn.weight', 'conv4.1.conv2.3.layers.0.bn.bias', 'conv4.1.conv2.3.layers.0.bn.running_mean', 'conv4.1.conv2.3.layers.0.bn.running_var', 'conv4.1.conv2.3.layers.0.bn.num_batches_tracked', 'conv4.1.conv2.3.layers.1.conv1.weight', 'conv4.1.conv2.3.layers.1.conv2.weight', 'conv4.1.conv2.3.layers.1.bn.weight', 'conv4.1.conv2.3.layers.1.bn.bias', 'conv4.1.conv2.3.layers.1.bn.running_mean', 'conv4.1.conv2.3.layers.1.bn.running_var', 'conv4.1.conv2.3.layers.1.bn.num_batches_tracked', 'conv4.1.conv2.3.layers.2.conv1.weight', 'conv4.1.conv2.3.layers.2.conv2.weight', 'conv4.1.conv2.3.layers.2.bn.weight', 'conv4.1.conv2.3.layers.2.bn.bias', 'conv4.1.conv2.3.layers.2.bn.running_mean', 'conv4.1.conv2.3.layers.2.bn.running_var', 'conv4.1.conv2.3.layers.2.bn.num_batches_tracked', 'conv4.1.conv2.3.layers.3.conv1.weight', 'conv4.1.conv2.3.layers.3.conv2.weight', 'conv4.1.conv2.3.layers.3.bn.weight', 'conv4.1.conv2.3.layers.3.bn.bias', 'conv4.1.conv2.3.layers.3.bn.running_mean', 'conv4.1.conv2.3.layers.3.bn.running_var', 'conv4.1.conv2.3.layers.3.bn.num_batches_tracked', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159ca8150d6641178d910b9141d63407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequence seq_024\n",
      "  Processing camera 1\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt to 'yolov8x.pt': 100% ━━━━━━━━━━━━ 130.5MB 162.3MB/s 0.8s\n",
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n",
      "Successfully loaded pretrained weights from \"./models/osnet_x1_0_market_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "    Processing frame 0\n",
      "    Processing frame 100\n",
      "    Processing frame 200\n",
      "    Processing frame 300\n",
      "    Processing frame 400\n",
      "    Processing frame 500\n",
      "    Processing frame 600\n",
      "    Processing frame 700\n",
      "    Processing frame 800\n",
      "    Processing frame 900\n",
      "    Processing frame 1000\n",
      "    Processing frame 1100\n",
      "    Processing frame 1200\n",
      "    Processing frame 1300\n",
      "    Processing frame 1400\n",
      "    Processing frame 1500\n",
      "    Processing frame 1600\n",
      "    Processing frame 1700\n",
      "    Processing frame 1800\n",
      "    Processing frame 1900\n",
      "    Processing frame 2000\n",
      "    Processing frame 2100\n",
      "    Processing frame 2200\n",
      "    Processing frame 2300\n",
      "    Processing frame 2400\n",
      "    Processing frame 2500\n",
      "    Processing frame 2600\n",
      "    Processing frame 2700\n",
      "    Processing frame 2800\n",
      "    Processing frame 2900\n",
      "    Processing frame 3000\n",
      "    Processing frame 3100\n",
      "    Processing frame 3200\n",
      "    Processing frame 3300\n",
      "    Processing frame 3400\n",
      "    Processing frame 3500\n",
      "    Processing frame 3600\n",
      "    Processing frame 3700\n",
      "    Processing frame 3800\n",
      "    Processing frame 3900\n",
      "    Processing frame 4000\n",
      "    Processing frame 4100\n",
      "    Processing frame 4200\n",
      "    Processing frame 4300\n",
      "    Processing frame 4400\n",
      "    Processing frame 4500\n",
      "    Processing frame 4600\n",
      "    Processing frame 4700\n",
      "    Processing frame 4800\n",
      "    Processing frame 4900\n",
      "    Processing frame 5000\n",
      "    Processing frame 5100\n",
      "    Processing frame 5200\n",
      "    Processing frame 5300\n",
      "    Processing frame 5400\n",
      "    Processing frame 5500\n",
      "    Processing frame 5600\n",
      "    Processing frame 5700\n",
      "    Processing frame 5800\n",
      "    Processing frame 5900\n",
      "    Processing frame 6000\n",
      "    Processing frame 6100\n",
      "    Processing frame 6200\n",
      "    Processing frame 6300\n",
      "    Processing frame 6400\n",
      "    Processing frame 6500\n",
      "    Processing frame 6600\n",
      "    Processing frame 6700\n",
      "    Processing frame 6800\n",
      "    Processing frame 6900\n",
      "    Processing frame 7000\n",
      "    Processing frame 7100\n",
      "    Processing frame 7200\n",
      "    Processing frame 7300\n",
      "    Processing frame 7400\n",
      "    Processing frame 7500\n",
      "    Processing frame 7600\n",
      "    Processing frame 7700\n",
      "    Processing frame 7800\n",
      "    Processing frame 7900\n",
      "    Processing frame 8000\n",
      "    Processing frame 8100\n",
      "    Processing frame 8200\n",
      "    Processing frame 8300\n",
      "    Processing frame 8400\n",
      "    Processing frame 8500\n",
      "    Processing frame 8600\n",
      "    Processing frame 8700\n",
      "    Processing frame 8800\n",
      "    Processing frame 8900\n",
      "    Processing frame 9000\n",
      "    Processing frame 9100\n",
      "    Processing frame 9200\n",
      "    Processing frame 9300\n",
      "    Processing frame 9400\n",
      "    Processing frame 9500\n",
      "    Processing frame 9600\n",
      "    Processing frame 9700\n",
      "    Processing frame 9800\n",
      "    Processing frame 9900\n",
      "    Processing frame 10000\n",
      "    Processing frame 10100\n",
      "    Processing frame 10200\n",
      "    Processing frame 10300\n",
      "    Processing frame 10400\n",
      "    Processing frame 10500\n",
      "    Processing frame 10700\n",
      "    Processing frame 10800\n",
      "    Processing frame 10900\n",
      "    Processing frame 11000\n",
      "    Processing frame 11100\n",
      "    Processing frame 11200\n",
      "    Processing frame 11400\n",
      "    Processing frame 11500\n",
      "    Processing frame 12000\n",
      "    Processing frame 12100\n",
      "    Processing frame 12200\n",
      "    Processing frame 12300\n",
      "    Processing frame 12400\n",
      "    Processing frame 12500\n",
      "    Processing frame 12600\n",
      "    Processing frame 12800\n",
      "    Processing frame 12900\n",
      "    Processing frame 13900\n",
      "    Processing frame 14900\n",
      "    Processing frame 15000\n",
      "    Processing frame 15100\n",
      "    Processing frame 15200\n",
      "    Processing frame 15300\n",
      "    Processing frame 15400\n",
      "    Processing frame 15600\n",
      "    Processing frame 15700\n",
      "    Processing frame 15800\n",
      "    Processing frame 15900\n",
      "    Processing frame 16400\n",
      "    Processing frame 16500\n",
      "    Processing frame 16600\n",
      "    Processing frame 16700\n",
      "    Processing frame 16800\n",
      "    Processing frame 16900\n",
      "Start SAMPLING and EMBEDDING\n",
      "Start SAVING metadata\n",
      "Start SAVING embeddings\n",
      "Start SAVING video\n",
      "  Processing camera 2\n",
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n",
      "Successfully loaded pretrained weights from \"./models/osnet_x1_0_market_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "    Processing frame 0\n",
      "    Processing frame 100\n",
      "    Processing frame 200\n",
      "    Processing frame 300\n",
      "    Processing frame 400\n",
      "    Processing frame 500\n",
      "    Processing frame 600\n",
      "    Processing frame 700\n",
      "    Processing frame 800\n",
      "    Processing frame 900\n",
      "    Processing frame 1000\n",
      "    Processing frame 1100\n",
      "    Processing frame 1200\n",
      "    Processing frame 1300\n",
      "    Processing frame 1400\n",
      "    Processing frame 1500\n",
      "    Processing frame 1600\n",
      "    Processing frame 1700\n",
      "    Processing frame 1800\n",
      "    Processing frame 1900\n",
      "    Processing frame 2000\n",
      "    Processing frame 2100\n",
      "    Processing frame 2200\n",
      "    Processing frame 2300\n",
      "    Processing frame 2400\n",
      "    Processing frame 2500\n",
      "    Processing frame 2600\n",
      "    Processing frame 2700\n",
      "    Processing frame 2800\n",
      "    Processing frame 2900\n",
      "    Processing frame 3000\n",
      "    Processing frame 3100\n",
      "    Processing frame 3200\n",
      "    Processing frame 3300\n",
      "    Processing frame 3400\n",
      "    Processing frame 3500\n",
      "    Processing frame 3600\n",
      "    Processing frame 3700\n",
      "    Processing frame 3800\n",
      "    Processing frame 3900\n",
      "    Processing frame 4000\n",
      "    Processing frame 4100\n",
      "    Processing frame 4200\n",
      "    Processing frame 4300\n",
      "    Processing frame 4400\n",
      "    Processing frame 4500\n",
      "    Processing frame 4600\n",
      "    Processing frame 4700\n",
      "    Processing frame 4800\n",
      "    Processing frame 4900\n",
      "    Processing frame 5000\n",
      "    Processing frame 5100\n",
      "    Processing frame 5200\n",
      "    Processing frame 5300\n",
      "    Processing frame 5400\n",
      "    Processing frame 5500\n",
      "    Processing frame 5600\n",
      "    Processing frame 5700\n",
      "    Processing frame 5800\n",
      "    Processing frame 5900\n",
      "    Processing frame 6000\n",
      "    Processing frame 6100\n",
      "    Processing frame 6200\n",
      "    Processing frame 6300\n",
      "    Processing frame 6400\n",
      "    Processing frame 6500\n",
      "    Processing frame 6600\n",
      "    Processing frame 6700\n",
      "    Processing frame 6800\n",
      "    Processing frame 6900\n",
      "    Processing frame 7000\n",
      "    Processing frame 7100\n",
      "    Processing frame 7200\n",
      "    Processing frame 7300\n",
      "    Processing frame 7400\n",
      "    Processing frame 7500\n",
      "    Processing frame 7600\n",
      "    Processing frame 7700\n",
      "    Processing frame 7800\n",
      "    Processing frame 7900\n",
      "    Processing frame 8000\n",
      "    Processing frame 8100\n",
      "    Processing frame 8200\n",
      "    Processing frame 8300\n",
      "    Processing frame 8400\n",
      "    Processing frame 8500\n",
      "    Processing frame 8600\n",
      "    Processing frame 8700\n",
      "    Processing frame 8800\n",
      "    Processing frame 8900\n",
      "    Processing frame 9000\n",
      "    Processing frame 9100\n",
      "    Processing frame 9200\n",
      "    Processing frame 9300\n",
      "    Processing frame 9400\n",
      "    Processing frame 9500\n",
      "    Processing frame 9600\n",
      "    Processing frame 9700\n",
      "    Processing frame 9800\n",
      "    Processing frame 9900\n",
      "    Processing frame 10000\n",
      "    Processing frame 10100\n",
      "    Processing frame 10200\n",
      "    Processing frame 10300\n",
      "    Processing frame 10400\n",
      "    Processing frame 10500\n",
      "    Processing frame 10600\n",
      "    Processing frame 10700\n",
      "    Processing frame 10800\n",
      "    Processing frame 10900\n",
      "    Processing frame 11000\n",
      "    Processing frame 11100\n",
      "    Processing frame 11200\n",
      "    Processing frame 11300\n",
      "    Processing frame 11400\n",
      "    Processing frame 11500\n",
      "    Processing frame 11600\n",
      "    Processing frame 11700\n",
      "    Processing frame 11800\n",
      "    Processing frame 11900\n",
      "    Processing frame 12000\n",
      "    Processing frame 12100\n",
      "    Processing frame 12200\n",
      "    Processing frame 12300\n",
      "    Processing frame 12400\n",
      "    Processing frame 12500\n",
      "    Processing frame 12600\n",
      "    Processing frame 12700\n",
      "    Processing frame 12800\n",
      "    Processing frame 12900\n",
      "    Processing frame 13000\n",
      "    Processing frame 13100\n",
      "    Processing frame 13200\n",
      "    Processing frame 13300\n",
      "    Processing frame 13400\n",
      "    Processing frame 13500\n",
      "    Processing frame 13600\n",
      "    Processing frame 13700\n",
      "    Processing frame 13800\n",
      "    Processing frame 13900\n",
      "    Processing frame 14000\n",
      "    Processing frame 14100\n",
      "    Processing frame 14200\n",
      "    Processing frame 14300\n",
      "    Processing frame 14400\n",
      "    Processing frame 14500\n",
      "    Processing frame 14600\n",
      "    Processing frame 14700\n",
      "    Processing frame 14800\n",
      "    Processing frame 14900\n",
      "    Processing frame 15000\n",
      "    Processing frame 15100\n",
      "    Processing frame 15200\n",
      "    Processing frame 15300\n",
      "    Processing frame 15400\n",
      "    Processing frame 15500\n",
      "    Processing frame 15600\n",
      "    Processing frame 15700\n",
      "    Processing frame 15800\n",
      "    Processing frame 15900\n",
      "    Processing frame 16000\n",
      "    Processing frame 16100\n",
      "    Processing frame 16200\n",
      "    Processing frame 16300\n",
      "    Processing frame 16400\n",
      "    Processing frame 16500\n",
      "    Processing frame 16600\n",
      "    Processing frame 16700\n",
      "    Processing frame 16800\n",
      "    Processing frame 16900\n",
      "Start SAMPLING and EMBEDDING\n",
      "Start SAVING metadata\n",
      "Start SAVING embeddings\n",
      "Start SAVING video\n",
      "  Processing camera 3\n",
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n",
      "Successfully loaded pretrained weights from \"./models/osnet_x1_0_market_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "    Processing frame 0\n",
      "    Processing frame 100\n",
      "    Processing frame 200\n",
      "    Processing frame 300\n",
      "    Processing frame 400\n",
      "    Processing frame 500\n",
      "    Processing frame 600\n",
      "    Processing frame 700\n",
      "    Processing frame 800\n",
      "    Processing frame 900\n",
      "    Processing frame 1000\n",
      "    Processing frame 1100\n",
      "    Processing frame 1200\n",
      "    Processing frame 1300\n",
      "    Processing frame 1400\n",
      "    Processing frame 1500\n",
      "    Processing frame 1600\n",
      "    Processing frame 1700\n",
      "    Processing frame 1800\n",
      "    Processing frame 1900\n",
      "    Processing frame 2000\n",
      "    Processing frame 2100\n",
      "    Processing frame 2200\n",
      "    Processing frame 2300\n",
      "    Processing frame 2400\n",
      "    Processing frame 2500\n",
      "    Processing frame 2600\n",
      "    Processing frame 2700\n",
      "    Processing frame 2800\n",
      "    Processing frame 2900\n",
      "    Processing frame 3000\n",
      "    Processing frame 3100\n",
      "    Processing frame 3200\n",
      "    Processing frame 3300\n",
      "    Processing frame 3400\n",
      "    Processing frame 3500\n",
      "    Processing frame 3600\n",
      "    Processing frame 3700\n",
      "    Processing frame 3800\n",
      "    Processing frame 3900\n",
      "    Processing frame 4000\n",
      "    Processing frame 4100\n",
      "    Processing frame 4200\n",
      "    Processing frame 4300\n",
      "    Processing frame 4400\n",
      "    Processing frame 4500\n",
      "    Processing frame 4600\n",
      "    Processing frame 4700\n",
      "    Processing frame 4800\n",
      "    Processing frame 4900\n",
      "    Processing frame 5000\n",
      "    Processing frame 5100\n",
      "    Processing frame 5200\n",
      "    Processing frame 5300\n",
      "    Processing frame 5400\n",
      "    Processing frame 5500\n",
      "    Processing frame 5600\n",
      "    Processing frame 5700\n",
      "    Processing frame 5800\n",
      "    Processing frame 5900\n",
      "    Processing frame 6000\n",
      "    Processing frame 6100\n",
      "    Processing frame 6200\n",
      "    Processing frame 6300\n",
      "    Processing frame 6400\n",
      "    Processing frame 6500\n",
      "    Processing frame 6600\n",
      "    Processing frame 6700\n",
      "    Processing frame 6800\n",
      "    Processing frame 6900\n",
      "    Processing frame 7000\n",
      "    Processing frame 7100\n",
      "    Processing frame 7200\n",
      "    Processing frame 7300\n",
      "    Processing frame 7400\n",
      "    Processing frame 7500\n",
      "    Processing frame 7600\n",
      "    Processing frame 7700\n",
      "    Processing frame 7800\n",
      "    Processing frame 7900\n",
      "    Processing frame 8000\n",
      "    Processing frame 8100\n",
      "    Processing frame 8200\n",
      "    Processing frame 8300\n",
      "    Processing frame 8400\n",
      "    Processing frame 8500\n",
      "    Processing frame 8600\n",
      "    Processing frame 8700\n",
      "    Processing frame 8800\n",
      "    Processing frame 8900\n",
      "    Processing frame 9000\n",
      "    Processing frame 9100\n",
      "    Processing frame 9200\n",
      "    Processing frame 9300\n",
      "    Processing frame 9400\n",
      "    Processing frame 9500\n",
      "    Processing frame 9600\n",
      "    Processing frame 9700\n",
      "    Processing frame 9800\n",
      "    Processing frame 9900\n",
      "    Processing frame 10000\n",
      "    Processing frame 10100\n",
      "    Processing frame 10200\n",
      "    Processing frame 10300\n",
      "    Processing frame 10400\n",
      "    Processing frame 10500\n",
      "    Processing frame 10600\n",
      "    Processing frame 10700\n",
      "    Processing frame 10800\n",
      "    Processing frame 10900\n",
      "    Processing frame 11000\n",
      "    Processing frame 11100\n",
      "    Processing frame 11200\n",
      "    Processing frame 11300\n",
      "    Processing frame 11500\n",
      "    Processing frame 12100\n",
      "    Processing frame 12200\n",
      "    Processing frame 12400\n",
      "    Processing frame 12500\n",
      "    Processing frame 12600\n",
      "    Processing frame 12800\n",
      "    Processing frame 12900\n",
      "    Processing frame 13800\n",
      "    Processing frame 13900\n",
      "    Processing frame 14800\n",
      "    Processing frame 14900\n",
      "    Processing frame 15000\n",
      "    Processing frame 15100\n",
      "    Processing frame 15200\n",
      "    Processing frame 15300\n",
      "    Processing frame 15400\n",
      "    Processing frame 15500\n",
      "    Processing frame 15600\n",
      "    Processing frame 15700\n",
      "    Processing frame 15800\n",
      "    Processing frame 15900\n",
      "    Processing frame 16000\n",
      "    Processing frame 16100\n",
      "    Processing frame 16200\n",
      "    Processing frame 16300\n",
      "    Processing frame 16400\n",
      "    Processing frame 16500\n",
      "    Processing frame 16600\n",
      "    Processing frame 16700\n",
      "    Processing frame 16800\n",
      "    Processing frame 16900\n",
      "Start SAMPLING and EMBEDDING\n",
      "Start SAVING metadata\n",
      "Start SAVING embeddings\n",
      "Start SAVING video\n",
      "  Processing camera 4\n",
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n",
      "Successfully loaded pretrained weights from \"./models/osnet_x1_0_market_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "    Processing frame 10100\n",
      "    Processing frame 10600\n",
      "    Processing frame 11200\n",
      "    Processing frame 11400\n",
      "    Processing frame 11600\n",
      "    Processing frame 12100\n",
      "    Processing frame 12300\n",
      "    Processing frame 12400\n",
      "    Processing frame 12500\n",
      "    Processing frame 12600\n",
      "    Processing frame 12700\n",
      "    Processing frame 12800\n",
      "    Processing frame 12900\n",
      "    Processing frame 13000\n",
      "    Processing frame 13300\n",
      "    Processing frame 13700\n",
      "    Processing frame 14700\n",
      "    Processing frame 14800\n",
      "    Processing frame 14900\n",
      "    Processing frame 15700\n",
      "    Processing frame 15800\n",
      "    Processing frame 15900\n",
      "    Processing frame 16000\n",
      "    Processing frame 16100\n",
      "    Processing frame 16200\n",
      "    Processing frame 16300\n",
      "    Processing frame 16700\n",
      "Start SAMPLING and EMBEDDING\n",
      "Start SAVING metadata\n",
      "Start SAVING embeddings\n",
      "Start SAVING video\n",
      "  Processing camera 5\n",
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n",
      "Successfully loaded pretrained weights from \"./models/osnet_x1_0_market_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "    Processing frame 9800\n",
      "    Processing frame 9900\n",
      "    Processing frame 10200\n",
      "    Processing frame 10300\n",
      "    Processing frame 10400\n",
      "    Processing frame 10700\n",
      "    Processing frame 10800\n",
      "    Processing frame 10900\n",
      "    Processing frame 11000\n",
      "    Processing frame 11100\n",
      "    Processing frame 11200\n",
      "    Processing frame 11300\n",
      "    Processing frame 11400\n",
      "    Processing frame 11500\n",
      "    Processing frame 11600\n",
      "    Processing frame 11700\n",
      "    Processing frame 11800\n",
      "    Processing frame 11900\n",
      "    Processing frame 12000\n",
      "    Processing frame 12100\n",
      "    Processing frame 12200\n",
      "    Processing frame 12300\n",
      "    Processing frame 12700\n",
      "    Processing frame 12900\n",
      "    Processing frame 13000\n",
      "    Processing frame 13100\n",
      "    Processing frame 13200\n",
      "    Processing frame 13300\n",
      "    Processing frame 13400\n",
      "    Processing frame 13500\n",
      "    Processing frame 13600\n",
      "    Processing frame 13700\n",
      "    Processing frame 13800\n",
      "    Processing frame 14000\n",
      "    Processing frame 14700\n",
      "    Processing frame 15200\n",
      "    Processing frame 15300\n",
      "    Processing frame 15400\n",
      "    Processing frame 15500\n",
      "    Processing frame 15600\n",
      "    Processing frame 15700\n",
      "    Processing frame 15800\n",
      "    Processing frame 15900\n",
      "    Processing frame 16300\n",
      "    Processing frame 16400\n",
      "    Processing frame 16500\n",
      "    Processing frame 16800\n",
      "    Processing frame 16900\n",
      "Start SAMPLING and EMBEDDING\n",
      "Start SAVING metadata\n",
      "Start SAVING embeddings\n",
      "Start SAVING video\n",
      "  Processing camera 6\n",
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n",
      "Successfully loaded pretrained weights from \"./models/osnet_x1_0_market_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "    Processing frame 400\n",
      "    Processing frame 10300\n",
      "    Processing frame 10600\n",
      "    Processing frame 11400\n",
      "    Processing frame 11500\n",
      "    Processing frame 11700\n",
      "    Processing frame 11800\n",
      "    Processing frame 12000\n",
      "    Processing frame 12100\n",
      "    Processing frame 12800\n",
      "    Processing frame 13100\n",
      "    Processing frame 13800\n",
      "    Processing frame 13900\n",
      "    Processing frame 14100\n",
      "    Processing frame 14500\n",
      "    Processing frame 14600\n",
      "    Processing frame 15100\n",
      "    Processing frame 15300\n",
      "    Processing frame 15400\n",
      "    Processing frame 15500\n",
      "    Processing frame 15800\n",
      "    Processing frame 16100\n",
      "    Processing frame 16300\n",
      "    Processing frame 16400\n",
      "    Processing frame 16500\n",
      "    Processing frame 16600\n",
      "Start SAMPLING and EMBEDDING\n",
      "Start SAVING metadata\n",
      "Start SAVING embeddings\n",
      "Start SAVING video\n",
      "  Processing camera 7\n",
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n",
      "Successfully loaded pretrained weights from \"./models/osnet_x1_0_market_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "    Processing frame 0\n",
      "    Processing frame 100\n",
      "    Processing frame 200\n",
      "    Processing frame 300\n",
      "    Processing frame 400\n",
      "    Processing frame 500\n",
      "    Processing frame 600\n",
      "    Processing frame 10400\n",
      "    Processing frame 10500\n",
      "    Processing frame 11400\n",
      "    Processing frame 11500\n",
      "    Processing frame 11600\n",
      "    Processing frame 11700\n",
      "    Processing frame 11800\n",
      "    Processing frame 11900\n",
      "    Processing frame 12000\n",
      "    Processing frame 12900\n",
      "    Processing frame 13000\n",
      "    Processing frame 13900\n",
      "    Processing frame 14000\n",
      "    Processing frame 14100\n",
      "    Processing frame 14200\n",
      "    Processing frame 14300\n",
      "    Processing frame 14400\n",
      "    Processing frame 14500\n",
      "    Processing frame 14600\n",
      "    Processing frame 14700\n",
      "    Processing frame 14800\n",
      "    Processing frame 14900\n",
      "    Processing frame 15000\n",
      "    Processing frame 15100\n",
      "    Processing frame 15400\n",
      "    Processing frame 15500\n",
      "    Processing frame 15600\n",
      "    Processing frame 15900\n",
      "    Processing frame 16000\n",
      "    Processing frame 16100\n",
      "    Processing frame 16200\n",
      "    Processing frame 16300\n",
      "    Processing frame 16500\n",
      "Start SAMPLING and EMBEDDING\n",
      "Start SAVING metadata\n",
      "Start SAVING embeddings\n",
      "Start SAVING video\n"
     ]
    }
   ],
   "source": [
    "# tracking\n",
    "reid_model = ReIDModel(device=device, model_path='./models/osnet_ain_x1_0_dukemtmcreid_256x128_amsgrad_ep90_lr0.0015_coslr_b64_fb10_softmax_labsmth_flip_jitter.pth')\n",
    "clip_model = CLIPModel(device=device)\n",
    "\n",
    "seqs = sorted(glob.glob(f'{VIDEO_FOLDER}/seq_*'))\n",
    "\n",
    "# seq 002\n",
    "for seq in seqs[-3:-2]: # !!!!!\n",
    "    seq_name = os.path.basename(seq)\n",
    "    seq_id = int(seq_name.split('_')[-1])\n",
    "    print(f'Processing sequence {seq_name}')\n",
    "    \n",
    "    os.makedirs(os.path.join(OUTPUT_FOLDER, 'frames', seq_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_FOLDER, 'annotated_videos', seq_name), exist_ok=True)\n",
    "    cameras = sorted(glob.glob(f'{seq}/camera_*'))\n",
    "    for video_path in cameras: \n",
    "        manager = TrackletManager()\n",
    "        camera_name = \"_\".join(os.path.basename(video_path).split('_')[:2])\n",
    "        cam_id = int(camera_name[-1])\n",
    "        camera_frame_folder = os.path.join(OUTPUT_FOLDER, 'frames', seq_name, camera_name)\n",
    "        os.makedirs(camera_frame_folder, exist_ok=True)\n",
    "        print(f'  Processing camera {cam_id}')\n",
    "        for frame_id, frame, boxes, ids, confs in run_tracking(video_path, model_name='yolov8x.pt', \n",
    "                                                               vid_stride=1, \n",
    "                                                               confidence=CONFIDENCE_THRESHOLD,\n",
    "                                                               device=device):\n",
    "            if frame_id%100==0:\n",
    "                print(f'    Processing frame {frame_id}') \n",
    "            # detected boxes + (alive but not detected)\n",
    "            # print(boxes)\n",
    "            frame_save_path = os.path.join(camera_frame_folder, f'frame_{frame_id:06d}.webp')\n",
    "            # save_image_webp(frame, frame_save_path)\n",
    "            for box, tid, conf in zip(boxes, ids, confs):\n",
    "                # print(f'      Track ID: {tid}, BBox: {box}, Conf: {conf}')\n",
    "                gid = seq_id*SEQ_ID_OFFSET + cam_id * CAMERA_ID_OFFSET + tid\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # invalid box\n",
    "                if x2<=x1 and y2<=y1:\n",
    "                    continue\n",
    "                \n",
    "                crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "                crop_path = os.path.join(\n",
    "                    OUTPUT_FOLDER,\n",
    "                    \"crops\",\n",
    "                    seq_name,\n",
    "                    camera_name,\n",
    "                    f\"{gid}_{frame_id:06d}.webp\"\n",
    "                )\n",
    "                \n",
    "                save_crop_webp(crop, crop_path)\n",
    "                t = manager.get(gid, seq_id, cam_id)\n",
    "                t.add_frame(frame_id, box, conf, crop_path)\n",
    "\n",
    "        print('Start SAMPLING and EMBEDDING')\n",
    "        tracklets = manager.all()\n",
    "\n",
    "        for t in tracklets:\n",
    "            candidates = sample_best_per_window(t.frames)\n",
    "            \n",
    "            candidate_paths = set(f.crop_path for f in candidates)\n",
    "\n",
    "            # delete crops that are not used for CLIP & ReID embedding\n",
    "            for f in t.frames:\n",
    "                if f.crop_path not in candidate_paths:\n",
    "                    safe_delete(f.crop_path)\n",
    "                    f.crop_path = None\n",
    "\n",
    "            imgs = [cv2.imread(f.crop_path) for f in candidates]\n",
    "        \n",
    "            reid_feat = reid_model.extract(imgs).mean(axis=0)\n",
    "            reid_feat = reid_feat / np.linalg.norm(reid_feat)\n",
    "\n",
    "            t.reid_embeddings.append(reid_feat)\n",
    "\n",
    "            \n",
    "            clip_feat = clip_model.encode_batch(imgs).mean(axis=0)\n",
    "            clip_feat = clip_feat / np.linalg.norm(clip_feat)\n",
    "            \n",
    "            t.clip_embeddings.append(clip_feat)          \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        tracks = defaultdict(list)\n",
    "        \n",
    "        print('Start SAVING metadata')\n",
    "        # save into metadata.txt\n",
    "        with open(f\"{OUTPUT_FOLDER}/metadata/{seq_name}_{camera_name}.txt\", \"w\") as f:\n",
    "            for t in tracklets:\n",
    "                for frame in t.frames:\n",
    "                    f.write(f\"{t.sequence_id} {t.camera_id} {frame.frame_id} {t.global_id} {int(frame.bbox[0])} {int(frame.bbox[1])} {int(frame.bbox[2])} {int(frame.bbox[3])}\\n\")\n",
    "                    tracks[frame.frame_id].append({\n",
    "                        \"id\": t.global_id,\n",
    "                        \"bbox\": frame.bbox\n",
    "                    })\n",
    "        print('Start SAVING embeddings')            \n",
    "        # save into features.txt\n",
    "        features = {}\n",
    "        output_path_pkl = f\"{OUTPUT_FOLDER}/features/{seq_name}_{camera_name}.pkl\"\n",
    "        \n",
    "        with open(f\"{OUTPUT_FOLDER}/features/{seq_name}_{camera_name}.txt\", \"w\") as f:\n",
    "            for t in tracklets:\n",
    "                f.write(f\"{t.sequence_id} {t.camera_id} {t.global_id} {t.reid_embeddings[0].tolist()} {t.clip_embeddings[0].tolist()}\\n\")\n",
    "                key = (t.sequence_id, t.camera_id, t.global_id)\n",
    "        \n",
    "                reid_emb = t.reid_embeddings[0]\n",
    "                clip_emb = t.clip_embeddings[0]\n",
    "            \n",
    "                features[key] = [reid_emb, clip_emb]\n",
    "                \n",
    "        with open(output_path_pkl, \"wb\") as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Start SAVING video')\n",
    "        visualize_video_with_ids(video_path, tracks, f'{os.path.join(OUTPUT_FOLDER, 'annotated_videos', seq_name, camera_name)}.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41537b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T20:15:46.392827Z",
     "iopub.status.busy": "2026-01-11T20:15:46.392549Z",
     "iopub.status.idle": "2026-01-11T20:17:02.028117Z",
     "shell.execute_reply": "2026-01-11T20:17:02.027095Z"
    },
    "papermill": {
     "duration": 75.664567,
     "end_time": "2026-01-11T20:17:02.030086",
     "exception": false,
     "start_time": "2026-01-11T20:15:46.365519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!zip -rq /kaggle/working/results.zip /kaggle/working/outputs"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8665047,
     "sourceId": 13632798,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12417.712501,
   "end_time": "2026-01-11T20:17:05.373832",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-11T16:50:07.661331",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0da3de8857b24d0a8ffcc60dff92f21d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_774394f564bb4aea9be7ab1b63307660",
       "placeholder": "​",
       "style": "IPY_MODEL_943bda0a69214d1cbc3f4effd51f2213",
       "tabbable": null,
       "tooltip": null,
       "value": "open_clip_pytorch_model.bin: 100%"
      }
     },
     "159ca8150d6641178d910b9141d63407": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0da3de8857b24d0a8ffcc60dff92f21d",
        "IPY_MODEL_1d1df10e486849afbe6f47c98edaeadb",
        "IPY_MODEL_548f22a92aa948b1b9b930d8afb02334"
       ],
       "layout": "IPY_MODEL_92728964e17c415ea3ba813d6318c64e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1d1df10e486849afbe6f47c98edaeadb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b670817988764f0f91f2f79a7b2ebec7",
       "max": 3944659877.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_92427a604d5c45ef975145265ab770c9",
       "tabbable": null,
       "tooltip": null,
       "value": 3944659877.0
      }
     },
     "548f22a92aa948b1b9b930d8afb02334": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c2a7b0ccea574b19b68ce26a22b926bc",
       "placeholder": "​",
       "style": "IPY_MODEL_be37bd24cde943909f4471dda9073822",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.94G/3.94G [00:21&lt;00:00, 131MB/s]"
      }
     },
     "774394f564bb4aea9be7ab1b63307660": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "92427a604d5c45ef975145265ab770c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "92728964e17c415ea3ba813d6318c64e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "943bda0a69214d1cbc3f4effd51f2213": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b670817988764f0f91f2f79a7b2ebec7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be37bd24cde943909f4471dda9073822": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c2a7b0ccea574b19b68ce26a22b926bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
