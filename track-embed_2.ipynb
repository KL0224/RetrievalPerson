{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59c10bb-bfb1-409a-a7a5-e9e669e70f0c",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:37:18.682644Z",
     "iopub.status.busy": "2026-01-02T13:37:18.681942Z",
     "iopub.status.idle": "2026-01-02T13:37:19.495068Z",
     "shell.execute_reply": "2026-01-02T13:37:19.494193Z",
     "shell.execute_reply.started": "2026-01-02T13:37:18.682609Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RetrievalPerson'...\n",
      "remote: Enumerating objects: 53, done.\u001b[K\n",
      "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
      "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
      "remote: Total 53 (delta 9), reused 50 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (53/53), 9.32 MiB | 30.58 MiB/s, done.\n",
      "Resolving deltas: 100% (9/9), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/KL0224/RetrievalPerson -b pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8bdd7ff-c3d5-4889-b543-1793435cc90a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:37:47.682808Z",
     "iopub.status.busy": "2026-01-02T13:37:47.682110Z",
     "iopub.status.idle": "2026-01-02T13:38:02.351464Z",
     "shell.execute_reply": "2026-01-02T13:38:02.350562Z",
     "shell.execute_reply.started": "2026-01-02T13:37:47.682773Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting open_clip_torch\n",
      "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.23.0+cu126)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2025.11.3)\n",
      "Collecting ftfy (from open_clip_torch)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.36.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.6.2)\n",
      "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.20)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.4.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.14)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.2.1rc0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.11.12)\n",
      "Downloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ftfy, open_clip_torch\n",
      "Successfully installed ftfy-6.3.1 open_clip_torch-3.2.0\n",
      "Collecting torchreid\n",
      "  Downloading torchreid-0.2.5.tar.gz (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: torchreid\n",
      "  Building wheel for torchreid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torchreid: filename=torchreid-0.2.5-py3-none-any.whl size=144324 sha256=2989ed43c8dd3d5bc022b091919e74008b3382445e17b0e87fe8b9068b839445\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/86/ff/80a1b78a90df470cbb12c075bf189ad33f1a41a881cf9e9a09\n",
      "Successfully built torchreid\n",
      "Installing collected packages: torchreid\n",
      "Successfully installed torchreid-0.2.5\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.246-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
      "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
      "Downloading ultralytics-8.3.246-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: ultralytics-thop, ultralytics\n",
      "Successfully installed ultralytics-8.3.246 ultralytics-thop-2.0.18\n"
     ]
    }
   ],
   "source": [
    "!pip install open_clip_torch\n",
    "!pip install torchreid\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2e3e3c-0ddf-441e-baec-cadc4ee9ac43",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:38:05.441583Z",
     "iopub.status.busy": "2026-01-02T13:38:05.441204Z",
     "iopub.status.idle": "2026-01-02T13:38:05.447770Z",
     "shell.execute_reply": "2026-01-02T13:38:05.447078Z",
     "shell.execute_reply.started": "2026-01-02T13:38:05.441547Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/RetrievalPerson\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/RetrievalPerson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4701f7d7",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:38:10.320645Z",
     "iopub.status.busy": "2026-01-02T13:38:10.320341Z",
     "iopub.status.idle": "2026-01-02T13:38:10.325143Z",
     "shell.execute_reply": "2026-01-02T13:38:10.324587Z",
     "shell.execute_reply.started": "2026-01-02T13:38:10.320620Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "VIDEO_FOLDER = 'videos_test' #'../../input/dataset-person/videos'\n",
    "OUTPUT_FOLDER = 'outputs'\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_FOLDER, 'frames'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_FOLDER, 'metadata'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_FOLDER, 'features'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cb92c",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:38:12.685259Z",
     "iopub.status.busy": "2026-01-02T13:38:12.684691Z",
     "iopub.status.idle": "2026-01-02T13:38:43.277017Z",
     "shell.execute_reply": "2026-01-02T13:38:43.276377Z",
     "shell.execute_reply.started": "2026-01-02T13:38:12.685232Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from config import *\n",
    "from tracking.tracklet import TrackletManager\n",
    "from tracking.detector_tracker import run_tracking\n",
    "from sampling.sampler import sample_best_per_window\n",
    "from models.reid import ReIDModel\n",
    "from models.clip_model import CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03fb6cc9",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:38:43.278709Z",
     "iopub.status.busy": "2026-01-02T13:38:43.278446Z",
     "iopub.status.idle": "2026-01-02T13:38:43.283471Z",
     "shell.execute_reply": "2026-01-02T13:38:43.282702Z",
     "shell.execute_reply.started": "2026-01-02T13:38:43.278683Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_image_webp(img_bgr, path: str, quality: int = 80, resize_factor: float = 0.5):\n",
    "    if resize_factor != 1.0:\n",
    "        img_bgr = cv2.resize(img_bgr, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    img_pil.save(path, format=\"WEBP\", quality=quality)\n",
    "\n",
    "def save_crop_webp(crop, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    cv2.imwrite(\n",
    "        path,\n",
    "        crop,\n",
    "        [cv2.IMWRITE_WEBP_QUALITY, 100]  # 100 = lossless\n",
    "    )\n",
    "\n",
    "def safe_delete(path):\n",
    "    try:\n",
    "        if path and os.path.exists(path):\n",
    "            os.remove(path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to delete {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c102d4",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:38:43.285486Z",
     "iopub.status.busy": "2026-01-02T13:38:43.284614Z",
     "iopub.status.idle": "2026-01-02T13:38:43.298889Z",
     "shell.execute_reply": "2026-01-02T13:38:43.298258Z",
     "shell.execute_reply.started": "2026-01-02T13:38:43.285460Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "manager = TrackletManager()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848d6c4",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:39:12.805051Z",
     "iopub.status.busy": "2026-01-02T13:39:12.804722Z",
     "iopub.status.idle": "2026-01-02T13:40:39.597218Z",
     "shell.execute_reply": "2026-01-02T13:40:39.596129Z",
     "shell.execute_reply.started": "2026-01-02T13:39:12.805022Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tracking\n",
    "reid_model = ReIDModel(device=device, model_path='models/osnet_x1_0_market_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth')\n",
    "clip_model = CLIPModel(device=device)\n",
    "\n",
    "seqs = sorted(glob.glob(f'{VIDEO_FOLDER}/seq_*'))\n",
    "\n",
    "for seq in seqs[:2]:\n",
    "    seq_name = os.path.basename(seq)\n",
    "    seq_id = int(seq_name.split('_')[-1])\n",
    "    print(f'Processing sequence {seq_name}')\n",
    "    \n",
    "    os.makedirs(os.path.join(OUTPUT_FOLDER, 'frames', seq_name), exist_ok=True)\n",
    "    cameras = sorted(glob.glob(f'{seq}/camera_*'))\n",
    "    for cam_id, video_path in enumerate(cameras):\n",
    "        manager = TrackletManager()\n",
    "        cam_id += 1\n",
    "        camera_name = \"_\".join(os.path.basename(video_path).split('_')[:2])\n",
    "        camera_frame_folder = os.path.join(OUTPUT_FOLDER, 'frames', seq_name, camera_name)\n",
    "        os.makedirs(camera_frame_folder, exist_ok=True)\n",
    "        print(f'  Processing camera {cam_id}')\n",
    "        for frame_id, frame, boxes, ids, confs in run_tracking(video_path, model_name='yolov8n.pt', \n",
    "                                                               vid_stride=1, \n",
    "                                                               confidence=CONFIDENCE_THRESHOLD,\n",
    "                                                               device=device):\n",
    "            print(f'    Processing frame {frame_id}, detected {len(boxes)} persons') \n",
    "            # detected boxes + (alive but not detected)\n",
    "            print(boxes)\n",
    "            frame_save_path = os.path.join(camera_frame_folder, f'frame_{frame_id:06d}.webp')\n",
    "            save_image_webp(frame, frame_save_path)\n",
    "            for box, tid, conf in zip(boxes, ids, confs):\n",
    "                print(f'      Track ID: {tid}, BBox: {box}, Conf: {conf}')\n",
    "                gid = seq_id*SEQ_ID_OFFSET + cam_id * CAMERA_ID_OFFSET + tid\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # invalid box\n",
    "                if x2<=x1 and y2<=y1:\n",
    "                    continue\n",
    "                \n",
    "                crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "                crop_path = os.path.join(\n",
    "                    OUTPUT_FOLDER,\n",
    "                    \"crops\",\n",
    "                    seq_name,\n",
    "                    camera_name,\n",
    "                    f\"{gid}_{frame_id:06d}.webp\"\n",
    "                )\n",
    "                \n",
    "                save_crop_webp(crop, crop_path)\n",
    "                t = manager.get(gid, seq_id, cam_id)\n",
    "                t.add_frame(frame_id, box, conf, crop_path)\n",
    "    \n",
    "        tracklets = manager.all()\n",
    "\n",
    "        for t in tracklets:\n",
    "            candidates = sample_best_per_window(t.frames)\n",
    "            # candidates = []\n",
    "            # if len(sampled) <= 3:\n",
    "            #     candidates = sampled\n",
    "            # else:\n",
    "            #     candidates = sampled[:1] + sampled[-1:] + sampled[len(sampled)//2:len(sampled)//2+1]\n",
    "            \n",
    "            candidate_paths = set(f.crop_path for f in candidates)\n",
    "            for f in t.frames:\n",
    "                if f.crop_path not in candidate_paths:\n",
    "                    safe_delete(f.crop_path)\n",
    "                    f.crop_path = None\n",
    "\n",
    "            imgs = [cv2.imread(f.crop_path) for f in candidates]\n",
    "        \n",
    "            reid_feat = reid_model.extract(imgs).mean(axis=0)\n",
    "            reid_feat = reid_feat / np.linalg.norm(reid_feat)\n",
    "\n",
    "            t.reid_embeddings.append(reid_feat)\n",
    "\n",
    "            \n",
    "            clip_feats = clip_model.encode_batch(imgs).mean(axis=0)\n",
    "            clip_feats = clip_feats / np.linalg.norm(clip_feats)\n",
    "            \n",
    "            t.clip_embeddings.append(clip_feats)           \n",
    "        \n",
    "        \n",
    "\n",
    "        # save into metadata.txt\n",
    "        with open(f\"{OUTPUT_FOLDER}/metadata/{seq_name}_{camera_name}.txt\", \"w\") as f:\n",
    "            for t in tracklets:\n",
    "                for frame in t.frames:\n",
    "                    f.write(f\"{t.sequence_id} {t.camera_id} {frame.frame_id} {t.global_id} {int(frame.bbox[0])} {int(frame.bbox[1])} {int(frame.bbox[2])} {int(frame.bbox[3])}\\n\")\n",
    "\n",
    "        # save into features.txt\n",
    "        features = {}\n",
    "        output_path_pkl = f\"{OUTPUT_FOLDER}/features/{seq_name}_{camera_name}.pkl\"\n",
    "        \n",
    "        with open(f\"{OUTPUT_FOLDER}/features/{seq_name}_{camera_name}.txt\", \"w\") as f:\n",
    "            for t in tracklets:\n",
    "                f.write(f\"{t.sequence_id} {t.camera_id} {t.global_id} {t.reid_embeddings[0].tolist()} {t.clip_embeddings[0].tolist()}\\n\")\n",
    "                key = (t.sequence_id, t.camera_id, t.global_id)\n",
    "        \n",
    "                reid_emb = t.reid_embeddings[0]\n",
    "                clip_emb = t.clip_embeddings[0]\n",
    "            \n",
    "                features[key] = [reid_emb, clip_emb]\n",
    "                \n",
    "        with open(output_path_pkl, \"wb\") as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb2e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracklets = manager.all()\n",
    "from collections import defaultdict\n",
    "\n",
    "tracks = defaultdict(list)\n",
    "bbox_per_frame = {}\n",
    "for t in tracklets:\n",
    "    for f in t.frames:\n",
    "        # print(t.global_id, f.frame_id, f.bbox)\n",
    "        # bbox_per_frame.setdefault(f.frame_id, []).append(f.bbox)\n",
    "        tracks[f.frame_id].append({\n",
    "                \"id\": t.global_id,\n",
    "                \"bbox\": f.bbox\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "\n",
    "def visualize_video_with_ids(\n",
    "    video_path,\n",
    "    tracks_per_frame,\n",
    "    output_path=None\n",
    "):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    writer = None\n",
    "    if output_path:\n",
    "        writer = cv2.VideoWriter(\n",
    "            output_path,\n",
    "            cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "            fps,\n",
    "            (width, height)\n",
    "        )\n",
    "\n",
    "    id_colors = {}\n",
    "\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        for item in tracks_per_frame.get(frame_idx, []):\n",
    "            track_id = item[\"id\"]\n",
    "            x1, y1, x2, y2 = map(int, item[\"bbox\"])\n",
    "\n",
    "            if track_id not in id_colors:\n",
    "                random.seed(int(track_id))\n",
    "                id_colors[track_id] = (\n",
    "                    random.randint(50, 255),\n",
    "                    random.randint(50, 255),\n",
    "                    random.randint(50, 255),\n",
    "                )\n",
    "\n",
    "            color = id_colors[track_id]\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"ID {track_id}\",\n",
    "                (x1, max(0, y1 - 7)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                color,\n",
    "                2\n",
    "            )\n",
    "\n",
    "        if writer:\n",
    "            writer.write(frame)\n",
    "\n",
    "        cv2.imshow(\"Tracking Visualization\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    if writer:\n",
    "        writer.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8092d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The only supported seed types are: None,\nint, float, str, bytes, and bytearray.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvisualize_video_with_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvideos_test/seq_001/camera_3_cut.mp4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mvisualize_video_with_ids\u001b[39m\u001b[34m(video_path, tracks_per_frame, output_path)\u001b[39m\n\u001b[32m     34\u001b[39m x1, y1, x2, y2 = \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, item[\u001b[33m\"\u001b[39m\u001b[33mbbox\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m track_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m id_colors:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     id_colors[track_id] = (\n\u001b[32m     39\u001b[39m         random.randint(\u001b[32m50\u001b[39m, \u001b[32m255\u001b[39m),\n\u001b[32m     40\u001b[39m         random.randint(\u001b[32m50\u001b[39m, \u001b[32m255\u001b[39m),\n\u001b[32m     41\u001b[39m         random.randint(\u001b[32m50\u001b[39m, \u001b[32m255\u001b[39m),\n\u001b[32m     42\u001b[39m     )\n\u001b[32m     44\u001b[39m color = id_colors[track_id]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/random.py:167\u001b[39m, in \u001b[36mRandom.seed\u001b[39m\u001b[34m(self, a, version)\u001b[39m\n\u001b[32m    164\u001b[39m     a = \u001b[38;5;28mint\u001b[39m.from_bytes(a + _sha512(a).digest())\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, (\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mThe only supported seed types are: None,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    168\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33mint, float, str, bytes, and bytearray.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28msuper\u001b[39m().seed(a)\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m.gauss_next = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: The only supported seed types are: None,\nint, float, str, bytes, and bytearray."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "visualize_video_with_ids('videos_test/seq_001/camera_3_cut.mp4', tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d26741",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:40:48.334460Z",
     "iopub.status.busy": "2026-01-02T13:40:48.333664Z",
     "iopub.status.idle": "2026-01-02T13:40:51.808713Z",
     "shell.execute_reply": "2026-01-02T13:40:51.808082Z",
     "shell.execute_reply.started": "2026-01-02T13:40:48.334424Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchreid/reid/utils/tools.py:43: UserWarning: No file found at \"osnet_x1_0_market_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth\"\n",
      "  warnings.warn('No file found at \"{}\"'.format(fpath))\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1LaG1EJpHrxdAxKnSCJ_i0u-nbxSAeiFY\n",
      "To: /root/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\n",
      "100%|██████████| 10.9M/10.9M [00:00<00:00, 179MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"/root/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n"
     ]
    }
   ],
   "source": [
    "# Sampling + Embeddings\n",
    "reid_model = ReIDModel()\n",
    "\n",
    "for t in manager.all():\n",
    "    sampled = sample_best_per_window(t.frames)\n",
    "    candidates = []\n",
    "    if len(sampled) <= 3:\n",
    "        candidates = sampled\n",
    "    else:\n",
    "        candidates = sampled[:1] + sampled[-1:] + sampled[len(sampled)//2:len(sampled)//2+1]\n",
    "\n",
    "    imgs = [f.image for f in candidates]\n",
    "\n",
    "    reid_feats = reid_model.extract(imgs).mean(axis=0)\n",
    "    t.reid_embeddings.append(reid_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e0c5b1",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:40:59.255020Z",
     "iopub.status.busy": "2026-01-02T13:40:59.254701Z",
     "iopub.status.idle": "2026-01-02T13:40:59.260707Z",
     "shell.execute_reply": "2026-01-02T13:40:59.260141Z",
     "shell.execute_reply.started": "2026-01-02T13:40:59.254989Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del reid_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33e908-f114-4430-98de-3221529c3c48",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea83320",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:41:03.602212Z",
     "iopub.status.busy": "2026-01-02T13:41:03.601891Z",
     "iopub.status.idle": "2026-01-02T13:42:41.619403Z",
     "shell.execute_reply": "2026-01-02T13:42:41.618739Z",
     "shell.execute_reply.started": "2026-01-02T13:41:03.602183Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a97cf90ac5042d2bb8edc15af1b07a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sampling + Embeddings\n",
    "clip_model = CLIPModel()\n",
    "\n",
    "for t in manager.all():\n",
    "    sampled = sample_best_per_window(t.frames)\n",
    "    candidates = []\n",
    "    if len(sampled) <= 3:\n",
    "        candidates = sampled\n",
    "    else:\n",
    "        candidates = sampled[:1] + sampled[-1:] + sampled[len(sampled)//2:len(sampled)//2+1]\n",
    "\n",
    "    imgs = [f.image for f in candidates]\n",
    "\n",
    "    clip_feats = np.array([clip_model.encode_image(img) for img in imgs]).mean(axis=0)\n",
    "    t.clip_embeddings.append(clip_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bd12c4f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:43:29.869659Z",
     "iopub.status.busy": "2026-01-02T13:43:29.869374Z",
     "iopub.status.idle": "2026-01-02T13:43:29.875903Z",
     "shell.execute_reply": "2026-01-02T13:43:29.874974Z",
     "shell.execute_reply.started": "2026-01-02T13:43:29.869634Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clip_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/1072766387.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clip_model' is not defined"
     ]
    }
   ],
   "source": [
    "del clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1ac5a2c",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T13:43:41.758264Z",
     "iopub.status.busy": "2026-01-02T13:43:41.757539Z",
     "iopub.status.idle": "2026-01-02T13:43:41.778230Z",
     "shell.execute_reply": "2026-01-02T13:43:41.777543Z",
     "shell.execute_reply.started": "2026-01-02T13:43:41.758232Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tracklets = manager.all()\n",
    "\n",
    "# save into metadata.txt\n",
    "with open(f\"{OUTPUT_FOLDER}/metadata.txt\", \"a\") as f:\n",
    "    for t in tracklets:\n",
    "        for frame in t.frames:\n",
    "            f.write(f\"{t.sequence_id} {t.camera_id} {frame.frame_id} {t.global_id} {int(frame.bbox[0])} {int(frame.bbox[1])} {int(frame.bbox[2])} {int(frame.bbox[3])}\\n\")\n",
    "\n",
    "# save into features.txt\n",
    "with open(f\"{OUTPUT_FOLDER}/features.txt\", \"w\") as f:\n",
    "    for t in tracklets:\n",
    "        f.write(f\"{t.sequence_id} {t.camera_id} {t.global_id} {t.reid_embeddings[0].tolist()} {t.clip_embeddings[0].tolist()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61786a-de8d-4d93-98fc-e84604ba0a1b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r results.zip /kaggle/working/outputs\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'results.zip')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8665047,
     "sourceId": 13632798,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
